{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pankajtandon/Gist/blob/main/gist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y2oZUyJSAaL"
      },
      "source": [
        "This notebook will show you a way to build your app iteratively in Colab.\n",
        "\n",
        "To run this notebook, navigate to \n",
        "https://colab.research.google.com\n",
        "and File | Open this notebook or simply click on the link above.\n",
        "\n",
        "\n",
        "To prevent your API Keyes from being committed to source control, do the following:\n",
        "- Create a directory in the root of your Google Drive and call it `colab_content`.\n",
        "- Create a file in that directory called `api-keys.txt` and in that file add contents like:\n",
        "```\n",
        "OPENAI_API_KEY=<your key>. \n",
        "NGROK_AUTH_TOKEN=<your key>\n",
        "```\n",
        "\n",
        "For OPENAI_API_KEY, you will need to create an account at https://platform.openai.com and it will cost you but it's usually pennies for moderate usage and usage can be monitored at https://platform.openai.com/account/usage\n",
        "The NGROK_AUTH_TOKEN is free and can be gotten from https://ngrok.com/\n",
        "\n",
        "\n",
        "Then run each cell in this notebook in order by looking at the comment in each cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AajPb-2VmqYB",
        "outputId": "a7441350-ee4a-4803-9929-736061bde407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# First mount a directory in Google Drive. This will help keep your API Keys out of source control.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIzvaKoTm3Gd"
      },
      "outputs": [],
      "source": [
        "# This will need to be done everytime your VM disconnects.\n",
        "\n",
        "!pip install pyngrok\n",
        "!pip install streamlit\n",
        "!pip install openai\n",
        "!pip install langchain\n",
        "!pip install tiktoken\n",
        "!pip install sentence_transformers\n",
        "!pip install tiktoken\n",
        "!pip install PyPDF2\n",
        "!pip install faiss-cpu\n",
        "!pip install ipdb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shjc55UyeCMI"
      },
      "outputs": [],
      "source": [
        "# For debugging\n",
        "import ipdb\n",
        "# %pdb on\n",
        "# %env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D2sXW7Unmcm",
        "outputId": "d100afff-9c86-4b86-8dfe-eeea2c853b93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/colab_content/gist.py\n"
          ]
        }
      ],
      "source": [
        "# This writes the code to the VM on which this notebook runs.\n",
        "\n",
        "%%writefile /content/drive/MyDrive/colab_content/gist.py\n",
        "\n",
        "\n",
        "# from scipy import spatial\n",
        "# import ast  # for converting embeddings saved as strings back to arrays\n",
        "# import openai  # for calling the OpenAI API\n",
        "# import pandas as pd  # for storing text and embeddings data\n",
        "# import tiktoken  # for counting tokens\n",
        "import time\n",
        "import streamlit as st\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "GPT_MODEL = \"gpt-3.5-turbo\"\n",
        "PAGE_CONFIG = {\"page_title\": \"Hello baby!\", \"page_icon\": \"smiley\", \"layout\": \"centered\"}\n",
        "st.set_page_config(**PAGE_CONFIG)\n",
        "st.title(\"Welcome to our world of baby delights!\")\n",
        "st.subheader(\"We are head over heels!\")\n",
        "\n",
        "\n",
        "# ---- User input\n",
        "\n",
        "st.write(\"The supplied PDF file (below) will be chunked and vectorized.\")\n",
        "chunk_size = st.slider('What should be the chunk size in characters? (if not sure, accept the default)', 0, 5000, value = 500, step = 25)\n",
        "chunk_overlap = st.slider('What should be the chunk overlap in characters? (if not sure, accept the default)', 0, 500, value = 100, step = 10)\n",
        "\n",
        "embeddings_option = st.selectbox(\n",
        "    label = 'Which Embeddings engine to use?',\n",
        "    options= ['HuggingFaceEmbeddings - Free but slow', 'OpenAIEmbeddings - Fast but costs']\n",
        ")\n",
        "\n",
        "debug = st.checkbox(\"Would you like to see debug info?\")\n",
        "pdf = st.file_uploader(\"Upload your PDF\", type = \"PDF\")\n",
        "question = None\n",
        "# -------\n",
        "\n",
        "if ((pdf is not None)):\n",
        "    # User supplied a pdf doc\n",
        "    \n",
        "    if embeddings_option.startswith(\"HuggingFace\"):\n",
        "      embeddings = HuggingFaceEmbeddings()\n",
        "      st.write(\"Using HuggingFaceEmbeddings\")\n",
        "    else:\n",
        "      embeddings = OpenAIEmbeddings()\n",
        "      st.write(\"Using OpenAIEmbeddings\")\n",
        "\n",
        "    st.write(\"Using chunk_size\", chunk_size, \"and chunk overlap of\", chunk_overlap)\n",
        "    pdf_reader = PdfReader(pdf)\n",
        "    content = \"\"\n",
        "    for page in pdf_reader.pages:\n",
        "        content += page.extract_text()\n",
        "    # st.write(\"====Content====\")\n",
        "    # st.write(content)\n",
        "\n",
        "    total_execution_seconds = 0;\n",
        "    # Chunk out the file\n",
        "    st.write(\"Going to split the file into chunks\")\n",
        "    start_time_for_chunking = time.time()\n",
        "    text_splitter = CharacterTextSplitter(separator=\" \", chunk_size= chunk_size, chunk_overlap = chunk_overlap, length_function= len)\n",
        "    chunks = text_splitter.split_text(content)\n",
        "    diff = (time.time() - start_time_for_chunking)\n",
        "    total_execution_seconds += diff\n",
        "    st.write(\"Split file into \", len(chunks), \" chunks\", \" in \", diff, \" seconds\")\n",
        "    \n",
        "    #ipdb.set_trace()\n",
        "    #Ask the question\n",
        "    question = st.text_input(\"Ask me something about the PDF that you just uploaded:\")\n",
        "    if len(question) > 0:\n",
        "        # These are the vectorized chunks:\n",
        "        start_time_for_vectorization = time.time()\n",
        "        st.write(\"Going to start vectorizing the chunks\")\n",
        "        knowledge_base = FAISS.from_texts(chunks, embeddings)\n",
        "        diff = (time.time() - start_time_for_vectorization)\n",
        "        total_execution_seconds += diff\n",
        "        st.write(\"Vectorization took %s seconds\" % diff)\n",
        "\n",
        "        # Docs are those vectors that are similar to the vectors in the knowledge base.\n",
        "        start_time_for_similarity_search = time.time()\n",
        "        docs = knowledge_base.similarity_search(question)\n",
        "        diff = (time.time() - start_time_for_similarity_search)\n",
        "        total_execution_seconds += diff\n",
        "        st.write(\"Similarity search took %s seconds\" % diff)\n",
        "\n",
        "        if docs is not None:\n",
        "            if (debug):\n",
        "              st.write(\"These are the related chunks:\")\n",
        "              for doc in docs:\n",
        "                st.write(doc)\n",
        "            \n",
        "            # Forward the related chunks to the LLM with the query as a prompt\n",
        "            llm = OpenAI()\n",
        "            st.write(\"Asking LLM using model\", llm.model_name, \"...\")\n",
        "            start_time_for_llm_question = time.time()    \n",
        "            chain = load_qa_chain(llm, chain_type = \"stuff\")\n",
        "            with get_openai_callback() as cb:\n",
        "                response = chain.run(question = question, input_documents = docs)\n",
        "                st.write(\"Cost of query:\")\n",
        "                st.write(cb)\n",
        "\n",
        "            diff = (time.time() - start_time_for_llm_question)\n",
        "            total_execution_seconds += diff\n",
        "            st.write(\"LLM response took %s seconds\" % diff)\n",
        "            st.write(response)\n",
        "            st.write(\"Total execution: %s seconds\" % total_execution_seconds)\n",
        "        else:\n",
        "            st.write(\"No match on the chunks!\")\n",
        "\n",
        "\n",
        "# EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjCJuZHOngpy",
        "outputId": "7110fb65-ac99-48b5-9603-82b62eebd25e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-06-13T21:49:45+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the URL that can be used to access the Streamlit app NgrokTunnel: \"https://48c3-34-86-3-59.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "# Set up the tunnel to allow access to the running Streamlit instance.\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "with open('/content/drive/MyDrive/colab_content/api-keys.txt', 'r') as f:\n",
        "    api_key_list = f.readlines()\n",
        "for kv in api_key_list:\n",
        "    k,v = kv.split('=')\n",
        "    #print(k, v)\n",
        "    os.environ[k] = v.strip()\n",
        "ngrok_token = os.getenv('NGROK_AUTH_TOKEN').strip()\n",
        "!ngrok authtoken $ngrok_token\n",
        "public_url = ngrok.connect(addr='8501') # This is the default Streamlit port\n",
        "print('This is the URL that can be used to access the Streamlit app', public_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgLS6Yg-b2k2",
        "outputId": "446e232a-adfb-464b-c3f0-6e4384b3ce91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.86.3.59:8501\u001b[0m\n",
            "\u001b[0m\n",
            "> \u001b[0;32m/content/drive/MyDrive/colab_content/gist.py\u001b[0m(73)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     72 \u001b[0;31m    \u001b[0;31m#Ask the question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 73 \u001b[0;31m    \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ask me something about the PDF that you just uploaded:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     74 \u001b[0;31m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Q\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"question is None\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[6n\u001b[?2004h\u001b[?1l\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mipdb> \u001b[6D\u001b[6C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[6D\u001b[J\u001b[0m\u001b[?7h\u001b[?2004lWARNING: your terminal doesn't support cursor position requests (CPR).\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mipdb> \u001b[6D\u001b[6C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[6D\u001b[0m\u001b[J\u001b[0;38;5;28mipdb> \u001b[0mc\u001b[7D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004lQ  question is None False\n",
            "> \u001b[0;32m/content/drive/MyDrive/colab_content/gist.py\u001b[0m(73)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     72 \u001b[0;31m    \u001b[0;31m#Ask the question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 73 \u001b[0;31m    \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ask me something about the PDF that you just uploaded:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     74 \u001b[0;31m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Q\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"question is None\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[6n\u001b[?2004h\u001b[?1l\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mipdb> \u001b[6D\u001b[6C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[6D\u001b[J\u001b[0m\u001b[?7h\u001b[?2004lWARNING: your terminal doesn't support cursor position requests (CPR).\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mipdb> \u001b[6D\u001b[6C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[6D\u001b[0m\u001b[J\u001b[0;38;5;28mipdb> \u001b[0mc\u001b[7D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004lQ What is my motility question is None False\n",
            "c\n",
            "r\n",
            "> \u001b[0;32m/content/drive/MyDrive/colab_content/gist.py\u001b[0m(73)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     72 \u001b[0;31m    \u001b[0;31m#Ask the question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 73 \u001b[0;31m    \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ask me something about the PDF that you just uploaded:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     74 \u001b[0;31m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Q\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"question is None\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[6n\u001b[?2004h\u001b[?1l\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mipdb> \u001b[6D\u001b[6C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[6D\u001b[0m\u001b[J\u001b[0;38;5;28mipdb> \u001b[0mc\u001b[7D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004lQ What is my motility question is None False\n",
            "2023-06-13 21:51:50.389 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 552, in _run_script\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/drive/MyDrive/colab_content/gist.py\", line 79, in <module>\n",
            "    knowledge_base = FAISS.from_texts(chunks, embeddings)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/faiss.py\", line 502, in from_texts\n",
            "    return cls.__from(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/faiss.py\", line 453, in __from\n",
            "    index = faiss.IndexFlatL2(len(embeddings[0]))\n",
            "IndexError: list index out of range\n",
            "> \u001b[0;32m/content/drive/MyDrive/colab_content/gist.py\u001b[0m(73)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     72 \u001b[0;31m    \u001b[0;31m#Ask the question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 73 \u001b[0;31m    \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ask me something about the PDF that you just uploaded:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     74 \u001b[0;31m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Q\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"question is None\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[?2004h\u001b[?1l\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mipdb> \u001b[0mr\u001b[7D\u001b[7C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[7D\u001b[0m\u001b[J\u001b[0;38;5;28mipdb> \u001b[0mr\u001b[7D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004lQ  question is None False\n",
            "--Return--\n",
            "None\n",
            "> \u001b[0;32m/content/drive/MyDrive/colab_content/gist.py\u001b[0m(75)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     74 \u001b[0;31m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Q\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"question is None\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 75 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     76 \u001b[0;31m        \u001b[0;31m# These are the vectorized chunks:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[6n\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mipdb> \u001b[6D\u001b[6C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[6D\u001b[J\u001b[0m\u001b[?7h\u001b[?2004lWARNING: your terminal doesn't support cursor position requests (CPR).\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mipdb> \u001b[6D\u001b[6C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h> \u001b[0;32m/content/drive/MyDrive/colab_content/gist.py\u001b[0m(73)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     72 \u001b[0;31m    \u001b[0;31m#Ask the question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 73 \u001b[0;31m    \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ask me something about the PDF that you just uploaded:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     74 \u001b[0;31m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Q\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"question is None\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[6n\u001b[?2004h\u001b[?1l\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mipdb> \u001b[6D\u001b[6C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[6D\u001b[J\u001b[0m\u001b[?7h\u001b[?2004lWARNING: your terminal doesn't support cursor position requests (CPR).\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mipdb> \u001b[6D\u001b[6C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?25l\u001b[?7l\u001b[6D\u001b[0m\u001b[J\u001b[0;38;5;28mipdb> \u001b[0mr\u001b[7D\u001b[0m\n",
            "\u001b[J\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[?2004lQ How much mortgage interest did I pay question is None False\n",
            "--Return--\n",
            "None\n",
            "> \u001b[0;32m/content/drive/MyDrive/colab_content/gist.py\u001b[0m(111)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    110 \u001b[0;31m            \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 111 \u001b[0;31m            \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total execution: %s seconds\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtotal_execution_seconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    112 \u001b[0;31m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[?2004h\u001b[?25l\u001b[0m\u001b[?7l\u001b[0m\u001b[J\u001b[0;38;5;28mipdb> \u001b[6D\u001b[6C\u001b[?7h\u001b[0m\u001b[?12l\u001b[?25h\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "Exception ignored in: <module 'threading' from '/usr/lib/python3.10/threading.py'>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1537, in _shutdown\n",
            "    atexit_call()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 31, in _python_exit\n",
            "    t.join()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/web/bootstrap.py\", line 69, in signal_handler\n",
            "    server.stop()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/web/server/server.py\", line 395, in stop\n",
            "    self._runtime.stop()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/runtime.py\", line 331, in stop\n",
            "    async_objs.eventloop.call_soon_threadsafe(stop_on_eventloop)\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 798, in call_soon_threadsafe\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 515, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n",
            "Exception ignored in atexit callback: <bound method InteractiveShell.atexit_operations of <IPython.terminal.interactiveshell.TerminalInteractiveShell object at 0x7f49e3d70fa0>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3908, in atexit_operations\n",
            "    self.history_manager.end_session()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/history.py\", line 574, in end_session\n",
            "    self.writeout_cache()\n",
            "  File \"<decorator-gen-46>\", line 2, in writeout_cache\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/history.py\", line 58, in needs_sqlite\n",
            "    return f(self, *a, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/history.py\", line 780, in writeout_cache\n",
            "    self._writeout_input_cache(conn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/history.py\", line 761, in _writeout_input_cache\n",
            "    with conn:\n",
            "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 139955468760832 and this is thread id 139956057073472.\n"
          ]
        }
      ],
      "source": [
        "# Start the streamlit app and leave it running and then access the running app at the URL above.\n",
        "\n",
        "!streamlit run /content/drive/MyDrive/colab_content/gist.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNeKQzB+VutIbCmNkJYw1/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}