{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxWcsM0ESSsm3tV9/yY7yx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pankajtandon/Gist/blob/main/gist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook will show you a way to build your app iteratively in Colab.\n",
        "\n",
        "To run this notebook, navigate to \n",
        "https://colab.research.google.com\n",
        "and File | Open this notebook or simply click on the link above.\n",
        "\n",
        "\n",
        "To prevent your API Keyes from being committed to source control, do the following:\n",
        "- Create a directory in the root of your Google Drive and call it `colab_content`.\n",
        "- Create a file in that directory called `api-keys.txt` and in that file add contents like:\n",
        "```\n",
        "OPENAI_API_KEY=<your key>. \n",
        "NGROK_AUTH_TOKEN=<your key>\n",
        "```\n",
        "\n",
        "For OPENAI_API_KEY, you will need to create an account at https://platform.openai.com and it will cost you but it's usually pennies for moderate usage and usage can be monitored at https://platform.openai.com/account/usage\n",
        "The NGROK_AUTH_TOKEN is free and can be gotten from https://ngrok.com/\n",
        "\n",
        "\n",
        "Then run each cell in this notebook in order by looking at the comment in each cell.\n",
        "\n"
      ],
      "metadata": {
        "id": "7y2oZUyJSAaL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AajPb-2VmqYB",
        "outputId": "9008150b-a37d-4d6e-fb50-36fa703126f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# First mount a directory in Google Drive. This will help keep your API Keys out of source control.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This will need to be done everytime your VM disconnects.\n",
        "\n",
        "!pip install pyngrok\n",
        "!pip install streamlit\n",
        "!pip install openai\n",
        "!pip install langchain\n",
        "!pip install tiktoken\n",
        "!pip install sentence_transformers\n",
        "!pip install tiktoken\n",
        "!pip install PyPDF2\n",
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "id": "EIzvaKoTm3Gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This writes the code to the VM on which this notebook runs.\n",
        "\n",
        "%%writefile /content/drive/MyDrive/colab_content/gist.py\n",
        "\n",
        "\n",
        "# from scipy import spatial\n",
        "# import ast  # for converting embeddings saved as strings back to arrays\n",
        "# import openai  # for calling the OpenAI API\n",
        "# import pandas as pd  # for storing text and embeddings data\n",
        "# import tiktoken  # for counting tokens\n",
        "import time\n",
        "import streamlit as st\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "GPT_MODEL = \"gpt-3.5-turbo\"\n",
        "PAGE_CONFIG = {\"page_title\": \"Hello baby!\", \"page_icon\": \"smiley\", \"layout\": \"centered\"}\n",
        "st.set_page_config(**PAGE_CONFIG)\n",
        "st.title(\"Welcome to our world of baby delights!\")\n",
        "st.subheader(\"We are head over heels!\")\n",
        "\n",
        "\n",
        "# ---- User input\n",
        "\n",
        "st.write(\"The supplied PDF file (below) will be chunked and vectorized.\")\n",
        "chunk_size = st.slider('What should be the chunk size in characters? (if not sure, accept the default)', 0, 5000, value = 500, step = 25)\n",
        "chunk_overlap = st.slider('What should be the chunk overlap in characters? (if not sure, accept the default)', 0, 500, value = 100, step = 10)\n",
        "\n",
        "embeddings_option = st.selectbox(\n",
        "    label = 'Which Embeddings engine to use?',\n",
        "    options= ['HuggingFaceEmbeddings - Free but slow', 'OpenAIEmbeddings - Fast but costs']\n",
        ")\n",
        "\n",
        "debug = st.checkbox(\"Would you like to see debug info?\")\n",
        "\n",
        "pdf = st.file_uploader(\"Upload your PDF\", type = \"PDF\")\n",
        "\n",
        "# -------\n",
        "\n",
        "if ((pdf is not None)):\n",
        "    # User supplied a pdf doc\n",
        "\n",
        "    if embeddings_option.startswith(\"HuggingFace\"):\n",
        "      embeddings = HuggingFaceEmbeddings()\n",
        "    else:\n",
        "      embeddings = OpenAIEmbeddings()\n",
        "\n",
        "    st.write(\"Using chunk_size\", chunk_size, \"and chunk overlap of\", chunk_overlap)\n",
        "    pdf_reader = PdfReader(pdf)\n",
        "    content = \"\"\n",
        "    for page in pdf_reader.pages:\n",
        "        content += page.extract_text()\n",
        "    # st.write(\"====Content====\")\n",
        "    # st.write(content)\n",
        "\n",
        "    # Chunk out the file\n",
        "    text_splitter = CharacterTextSplitter(separator=\" \", chunk_size= chunk_size, chunk_overlap = chunk_overlap, length_function= len)\n",
        "    chunks = text_splitter.split_text(content)\n",
        "\n",
        "    # st.write(\"====Chunks====\")\n",
        "    # st.write(chunks)\n",
        "\n",
        "    #Ask the question\n",
        "    question = st.text_input(\"Ask me something about the PDF that you just uploaded:\")\n",
        "    if question:\n",
        "        total_execution_seconds = 0;\n",
        "        # These are the vectorized chunks:\n",
        "        start_time_for_vectorization = time.time()\n",
        "        knowledge_base = FAISS.from_texts(chunks, embeddings)\n",
        "        diff = (time.time() - start_time_for_vectorization)\n",
        "        total_execution_seconds += diff\n",
        "        st.write(\"Vectorization took %s seconds\" % diff)\n",
        "\n",
        "        # Docs are those vectors that are similar to the vectors in the knowledge base.\n",
        "        start_time_for_similarity_search = time.time()\n",
        "        docs = knowledge_base.similarity_search(question)\n",
        "        diff = (time.time() - start_time_for_similarity_search)\n",
        "        total_execution_seconds += diff\n",
        "        st.write(\"Similarity search took %s seconds\" % diff)\n",
        "\n",
        "        if docs is not None:\n",
        "            if (debug):\n",
        "              st.write(\"These are the related chunks:\")\n",
        "              for doc in docs:\n",
        "                st.write(doc)\n",
        "            \n",
        "            # Forward the related chunks to the LLM with the query as a prompt\n",
        "            llm = OpenAI()\n",
        "            st.write(\"Using model\", llm.model_name)\n",
        "            start_time_for_llm_question = time.time()    \n",
        "            chain = load_qa_chain(llm, chain_type = \"stuff\")\n",
        "            with get_openai_callback() as cb:\n",
        "                response = chain.run(question = question, input_documents = docs)\n",
        "                st.write(\"Cost of query:\")\n",
        "                st.write(cb)\n",
        "\n",
        "            diff = (time.time() - start_time_for_llm_question)\n",
        "            total_execution_seconds += diff\n",
        "            st.write(\"LLM response took %s seconds\" % diff)\n",
        "            st.write(response)\n",
        "            st.write(\"Total execution: %s seconds\" % total_execution_seconds)\n",
        "        else:\n",
        "            st.write(\"No match on the chunks!\")\n",
        "\n",
        "\n",
        "# EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D2sXW7Unmcm",
        "outputId": "0cbb4b48-f72f-4339-8938-5ae50db0cfc9"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/colab_content/gist.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the tunnel to allow access to the running Streamlit instance.\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "with open('/content/drive/MyDrive/colab_content/api-keys.txt', 'r') as f:\n",
        "    api_key_list = f.readlines()\n",
        "for kv in api_key_list:\n",
        "    k,v = kv.split('=')\n",
        "    #print(k, v)\n",
        "    os.environ[k] = v.strip()\n",
        "ngrok_token = os.getenv('NGROK_AUTH_TOKEN').strip()\n",
        "!ngrok authtoken $ngrok_token\n",
        "public_url = ngrok.connect(addr='8501') # This is the default Streamlit port\n",
        "print('This is the URL that can be used to access the Streamlit app', public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjCJuZHOngpy",
        "outputId": "18cb778e-d75e-4485-d51a-33ad6a79e8e8"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-06-13T17:45:03+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the URL that can be used to access the Streamlit app NgrokTunnel: \"https://8432-34-86-3-59.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the streamlit app and leave it running and then access the running app at the URL above.\n",
        "\n",
        "!streamlit run /content/drive/MyDrive/colab_content/gist.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgLS6Yg-b2k2",
        "outputId": "b3620d60-37df-4c14-d176-39b46efa00fc"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.86.3.59:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}